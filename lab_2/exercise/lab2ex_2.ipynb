{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0b4eba54832caa312e872a59837a2001ff0db474df54dc33ac6c9fdf9b5b488fb",
   "display_name": "Python 3.9.1 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb129fbccf0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris.data', header=None)\n",
    "df = df.sample(frac=1) #shuffle\n",
    "\n",
    "# add label indices column\n",
    "mapping = {k: v for v, k in enumerate(df [4]. unique())}\n",
    "df[5] = df[4].map(mapping)\n",
    "\n",
    "# normalise data\n",
    "alldata = torch.tensor(df.iloc[:, [0,1,2,3]].values, dtype=torch.float)\n",
    "alldata = (alldata - alldata.mean(dim=0)) / alldata.var(dim=0)\n",
    "# create datasets\n",
    "targets_tr = torch.tensor(df.iloc[:100, 5].values, dtype=torch.long)\n",
    "targets_va = torch.tensor(df.iloc[100:, 5].values, dtype=torch.long)\n",
    "data_tr = alldata [:100]\n",
    "data_va = alldata [100:]\n"
   ]
  },
  {
   "source": [
    "# Task 2.1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(data, W1, W2, b1, b2):\n",
    "    return torch.relu(data @ W1 + b1) @ W2 + b2\n",
    "\n",
    "def train_mlp(data_tr: torch.Tensor, targets_tr: torch.Tensor, data_va: torch.Tensor, targets_va: torch.Tensor, num_epochs: int = 100, lr: int = 0.01):\n",
    "    # initialise weights and biases\n",
    "    W1 = torch.rand((4, 12), requires_grad=True)\n",
    "    W2 = torch.rand((12, 3), requires_grad=True)\n",
    "    b1 = torch.rand(1, requires_grad=True)\n",
    "    b2 = torch.rand(1, requires_grad=True)\n",
    "    loss_tr_iter = torch.empty(num_epochs)\n",
    "    loss_va_iter = torch.empty(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logits = mlp(data_tr, W1, W2, b1, b2)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, targets_tr, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        with torch.no_grad():\n",
    "            W1 -= lr * W1.grad\n",
    "            W2 -= lr * W2.grad\n",
    "            b1 -= lr * b1.grad\n",
    "            b2 -= lr * b2.grad\n",
    "\n",
    "        for params in [W1, W2, b1, b2]:\n",
    "            params.grad.zero_()\n",
    "\n",
    "        loss_tr_iter[epoch] = loss\n",
    "        loss_va_iter[epoch] = torch.nn.functional.cross_entropy(\n",
    "            mlp(data_va, W1, W2, b1, b2),\n",
    "            targets_va,\n",
    "            reduction=\"sum\"\n",
    "        )\n",
    "\n",
    "    return W1, W2, b1, b2, loss_tr_iter, loss_va_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(data, targets, W1, W2, b1, b2):\n",
    "    logits = mlp(data, W1, W2, b1, b2)\n",
    "    preds = torch.argmax(logits, axis=1)\n",
    "    acc = (targets == preds).sum() / preds.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accs = []\n",
    "val_accs = []\n",
    "loss_trs = []\n",
    "loss_vas = []\n",
    "\n",
    "for i in range(10):\n",
    "    W1, W2, b1, b2, loss_tr_iter, loss_va_iter = train_mlp(data_tr, targets_tr, data_va, targets_va)\n",
    "    loss_tr_iter = loss_tr_iter.detach().numpy()\n",
    "    loss_va_iter = loss_va_iter.detach().numpy()\n",
    "\n",
    "    acc_tr = accuracy(data_tr, targets_tr, W1, W2, b1, b2)\n",
    "    acc_va = accuracy(data_va, targets_va, W1, W2, b1, b2)\n",
    "    train_accs.append(acc_tr)\n",
    "    val_accs.append(acc_va)\n",
    "    loss_trs.append(loss_tr_iter)\n",
    "    loss_vas.append(loss_va_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor(0.9200),\n",
       " tensor(0.6600),\n",
       " tensor(0.9200),\n",
       " tensor(0.7100),\n",
       " tensor(0.9200),\n",
       " tensor(0.9200),\n",
       " tensor(0.8800),\n",
       " tensor(0.9500),\n",
       " tensor(0.8500),\n",
       " tensor(0.7700)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor(0.8800),\n",
       " tensor(0.6600),\n",
       " tensor(0.8800),\n",
       " tensor(0.7000),\n",
       " tensor(0.8800),\n",
       " tensor(0.8800),\n",
       " tensor(0.8200),\n",
       " tensor(0.8800),\n",
       " tensor(0.9000),\n",
       " tensor(0.7200)]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(3, 2, figsize=(7, 6), sharex=True, sharey=True)\n",
    "# for i in range(len(loss_trs)):\n",
    "#     j, k = i // 2, i % 2\n",
    "#     ax[j, k].plot(loss_trs[i])\n",
    "#     ax[j, k].plot(loss_vas[i])\n",
    "#     if i == 1:\n",
    "#         ax[j, k].legend([\"train\", \"validation\"])\n",
    "\n",
    "# fig.add_subplot(111, frameon=False)\n",
    "# plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig(\"report/Figures/loss_curves.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Training Accuracy  Validation Accuracy\n",
       "0               0.92                 0.88\n",
       "1               0.66                 0.66\n",
       "2               0.92                 0.88\n",
       "3               0.71                 0.70\n",
       "4               0.92                 0.88\n",
       "5               0.92                 0.88\n",
       "6               0.88                 0.82\n",
       "7               0.95                 0.88\n",
       "8               0.85                 0.90\n",
       "9               0.77                 0.72"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Accuracy</th>\n      <th>Validation Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.92</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.66</td>\n      <td>0.66</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.92</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.71</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.92</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.92</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.88</td>\n      <td>0.82</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.95</td>\n      <td>0.88</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.85</td>\n      <td>0.90</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.77</td>\n      <td>0.72</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "df_accs = pd.DataFrame({\"Training Accuracy\": train_accs, \"Validation Accuracy\": val_accs}, dtype=float)\n",
    "df_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\\begin{table}\n\\centering\n\\caption{Accuracies of repeated MLP training.}\n\\label{tab:accs}\n\\begin{tabular}{lrr}\n\\toprule\n{} &  Training Accuracy &  Validation Accuracy \\\\\n\\midrule\n0 &               0.92 &                 0.88 \\\\\n1 &               0.66 &                 0.66 \\\\\n2 &               0.92 &                 0.88 \\\\\n3 &               0.71 &                 0.70 \\\\\n4 &               0.92 &                 0.88 \\\\\n5 &               0.92 &                 0.88 \\\\\n6 &               0.88 &                 0.82 \\\\\n7 &               0.95 &                 0.88 \\\\\n8 &               0.85 &                 0.90 \\\\\n9 &               0.77 &                 0.72 \\\\\n\\bottomrule\n\\end{tabular}\n\\end{table}\n\n"
     ]
    }
   ],
   "source": [
    "print(df_accs.to_latex(caption=\"Accuracies of repeated MLP training.\", label=\"tab:accs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}