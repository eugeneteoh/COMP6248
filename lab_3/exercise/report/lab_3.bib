
@article{eckartApproximationOneMatrix1936,
  title = {The Approximation of One Matrix by Another of Lower Rank},
  author = {Eckart, Carl and Young, Gale},
  date = {1936-09-01},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {1},
  pages = {211--218},
  issn = {1860-0980},
  doi = {10.1007/BF02288367},
  url = {https://doi.org/10.1007/BF02288367},
  urldate = {2021-04-19},
  abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
  file = {/Users/eugene/Zotero/storage/LKRMCTNM/Eckart and Young - 1936 - The approximation of one matrix by another of lowe.pdf},
  langid = {english},
  number = {3}
}

@online{eldanPowerDepthFeedforward2016,
  title = {The {{Power}} of {{Depth}} for {{Feedforward Neural Networks}}},
  author = {Eldan, Ronen and Shamir, Ohad},
  date = {2016-05-08},
  url = {http://arxiv.org/abs/1512.03965},
  urldate = {2021-04-25},
  abstract = {We show that there is a simple (approximately radial) function on \$\textbackslash reals\^d\$, expressible by a small 3-layer feedforward neural networks, which cannot be approximated by any 2-layer network, to more than a certain constant accuracy, unless its width is exponential in the dimension. The result holds for virtually all known activation functions, including rectified linear units, sigmoids and thresholds, and formally demonstrates that depth -- even if increased by 1 -- can be exponentially more valuable than width for standard feedforward neural networks. Moreover, compared to related results in the context of Boolean functions, our result requires fewer assumptions, and the proof techniques and construction are very different.},
  archiveprefix = {arXiv},
  eprint = {1512.03965},
  eprinttype = {arxiv},
  file = {/Users/eugene/Zotero/storage/AW7PMPS4/Eldan and Shamir - 2016 - The Power of Depth for Feedforward Neural Networks.pdf;/Users/eugene/Zotero/storage/YJ8RVDBJ/1512.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryclass = {cs, stat},
  version = {4}
}

@software{teohEugeneteohCOMP6248DeepLearning2021,
  title = {Eugeneteoh/{{COMP6248}}-{{Deep}}-{{Learning}}},
  author = {Teoh, Eugene},
  date = {2021-04-19T21:51:27Z},
  origdate = {2021-02-11T13:22:12Z},
  url = {https://github.com/eugeneteoh/COMP6248-Deep-Learning},
  urldate = {2021-04-22},
  abstract = {University of Southampton COMP6248 Differentiable Programming (and Deep Learning) 2020-2021}
}

@online{WeightsBiases,
  title = {Weights \& {{Biases}}},
  url = {https://wandb.ai/site},
  urldate = {2021-04-26},
  abstract = {Weights \& Biases, developer tools for machine learning},
  file = {/Users/eugene/Zotero/storage/XZFVJMLA/home.html},
  langid = {english},
  organization = {{W\&B}}
}

@online{wilsonMarginalValueAdaptive2018,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  author = {Wilson, Ashia C. and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  date = {2018-05-21},
  url = {http://arxiv.org/abs/1705.08292},
  urldate = {2021-04-24},
  abstract = {Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.},
  archiveprefix = {arXiv},
  eprint = {1705.08292},
  eprinttype = {arxiv},
  file = {/Users/eugene/Zotero/storage/RHVHLZYF/Wilson et al. - 2018 - The Marginal Value of Adaptive Gradient Methods in.pdf;/Users/eugene/Zotero/storage/6CMTDIEI/1705.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}


